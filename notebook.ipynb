{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense,Dropout,TimeDistributed\n",
    "import glob\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(file_path):\n",
    "    df=pd.read_parquet(file_path)\n",
    "    sum=df.isnull().any().sum()\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usefulFeatures():\n",
    "    train_dir = \"data/train.parquet\"\n",
    "    df_useful = pd.read_csv(\"data/useful_columns.csv\")\n",
    "\n",
    "    null_counts = {col: 0 for col in df_useful.columns}\n",
    "    non_null_counts = {col: 0 for col in df_useful.columns}\n",
    "    null_percentages = {col: 0.0 for col in df_useful.columns}\n",
    "    for file in os.listdir(train_dir):\n",
    "        df_file = os.path.join(train_dir, file, \"part-0.parquet\")\n",
    "        if not os.path.exists(df_file):\n",
    "            continue\n",
    "\n",
    "        current_df = pd.read_parquet(df_file)\n",
    "        for useful_column in df_useful.columns:\n",
    "            if useful_column in current_df.columns:\n",
    "                null_value_count = current_df[useful_column].isnull().sum()\n",
    "                non_null_value_count = current_df[useful_column].notnull().sum()\n",
    "\n",
    "                null_counts[useful_column] += null_value_count\n",
    "                non_null_counts[useful_column] += non_null_value_count\n",
    "\n",
    "    # Calculate the percentage of null values\n",
    "    total_rows = sum(non_null_counts.values()) + sum(null_counts.values())\n",
    "    for col in df_useful.columns:\n",
    "        if col in null_counts:\n",
    "            null_percentages[col] = (null_counts[col] / total_rows) * 100\n",
    "\n",
    "    # Add the new rows to the DataFrame\n",
    "    df_useful.loc[\"Not Null Count\"] = non_null_counts\n",
    "    df_useful.loc[\"Null Percentage\"] = null_percentages\n",
    "\n",
    "    # Save the updated DataFrame back to CSV\n",
    "    df_useful.to_csv(\"data/useful_columns.csv\", index=False)\n",
    "    print(\"Null values, non-null counts, and null percentages added.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_empty_values():\n",
    "      train_dir = \"data/train.parquet\"\n",
    "      df_useful = pd.read_csv(\"data/useful_columns.csv\")\n",
    "      mean = {col: 0 for col in df_useful.columns}\n",
    "      sum={col: 0 for col in df_useful.columns}\n",
    "      total_rows=0\n",
    "      for file in os.listdir(train_dir):\n",
    "        df_file = os.path.join(train_dir, file, \"part-0.parquet\")\n",
    "        current_df = pd.read_parquet(df_file)\n",
    "        for useful_column in df_useful.columns:\n",
    "            if useful_column in current_df.columns:\n",
    "                sum_col=current_df[useful_column].sum()\n",
    "                sum[useful_column]+=sum_col\n",
    "        total_rows+=current_df.shape[0]\n",
    "      for useful_column in df_useful.columns:\n",
    "          mean[useful_column]=sum[useful_column]/total_rows\n",
    "      for file in os.listdir(train_dir):\n",
    "        df_file = os.path.join(train_dir, file, \"part-0.parquet\")\n",
    "        current_df = pd.read_parquet(df_file)\n",
    "        for useful_column in df_useful.columns:\n",
    "            if useful_column in current_df.columns:\n",
    "                current_df[useful_column]=current_df[useful_column].fillna(mean[useful_column])\n",
    "        current_df.to_parquet(df_file)\n",
    "      df_useful.loc[\"Mean\"] =mean\n",
    "      df_useful.to_csv(\"data/useful_columns.csv\",index=False)\n",
    "\n",
    "      print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaling():\n",
    "    train_dir = \"data/train.parquet\"\n",
    "    df_useful = pd.read_csv(\"data/useful_columns.csv\")\n",
    "    \n",
    "    # Initialize min and max values with extreme numbers\n",
    "    min_values = {col: float('inf') for col in df_useful.columns}\n",
    "    max_values = {col: float('-inf') for col in df_useful.columns}\n",
    "    \n",
    "    # Step 1: Find global min and max for each column\n",
    "    for file in os.listdir(train_dir):\n",
    "        df_file = os.path.join(train_dir, file, \"part-0.parquet\")\n",
    "        current_df = pd.read_parquet(df_file)\n",
    "        for useful_column in df_useful.columns:\n",
    "            if useful_column in current_df.columns:\n",
    "                current_min = current_df[useful_column].min()\n",
    "                current_max = current_df[useful_column].max()\n",
    "                min_values[useful_column] = min(min_values[useful_column], current_min)\n",
    "                max_values[useful_column] = max(max_values[useful_column], current_max)\n",
    "    \n",
    "    # Step 2: Scale each column in the dataset using Min-Max Scaling\n",
    "    for file in os.listdir(train_dir):\n",
    "        new_df_file=os.path.join(\"data/scaled\",file,\"part-0.parquet\")\n",
    "        if( not os.path.exists(new_df_file)):\n",
    "            new_df_dir = os.path.dirname(new_df_file)  # Extract the directory path\n",
    "            os.mkdir(new_df_dir)\n",
    "        print(file)\n",
    "        df_file = os.path.join(train_dir, file, \"part-0.parquet\")\n",
    "        current_df = pd.read_parquet(df_file)\n",
    "        for useful_column in df_useful.columns:\n",
    "            if useful_column in current_df.columns:\n",
    "                # Min-Max Scaling formula: (x - min) / (max - min)\n",
    "                min_val = min_values[useful_column]\n",
    "                max_val = max_values[useful_column]\n",
    "                if max_val > min_val:  # Avoid division by zero\n",
    "                    current_df[useful_column] = (\n",
    "                        current_df[useful_column] - min_val\n",
    "                    ) / (max_val - min_val)\n",
    "                else:\n",
    "                    # Handle the case where all values are the same\n",
    "                    current_df[useful_column] = 0\n",
    "        \n",
    "        # Save the scaled DataFrame back to the same Parquet file\n",
    "        current_df.to_parquet(new_df_file)\n",
    "    \n",
    "    print(\"Min-Max Scaling applied successfully!\")\n",
    "    print(\"Min values:\", min_values)\n",
    "    print(\"Max values:\", max_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pearson_correlation():\n",
    "    train_dir = \"data/train.parquet\"\n",
    "    df_useful = pd.read_csv(\"data/useful_columns.csv\")\n",
    "    num = {col: 0 for col in df_useful.columns}\n",
    "    pearson_correlations={col:0 for col in df_useful.columns}\n",
    "    deno_xi_Ux_2={col: 0 for col in df_useful.columns}\n",
    "    deno_yi_Uy_2={col: 0 for col in df_useful.columns}\n",
    "    for file in os.listdir(train_dir):\n",
    "        df_file = os.path.join(train_dir, file, \"part-0.parquet\")\n",
    "        current_df = pd.read_parquet(df_file)\n",
    "        for useful_column in df_useful.columns:\n",
    "            if useful_column in current_df.columns:\n",
    "                calculated_num_current_column=current_df[useful_column]\n",
    "                calculated_num_current_column=calculated_num_current_column.to_numpy()\n",
    "                target_column=current_df[\"responder_6\"].to_numpy()\n",
    "                calculated_num=np.sum((calculated_num_current_column-df_useful.loc[3,useful_column])*(target_column-df_useful.loc[3,\"responder_6\"]))\n",
    "                num[useful_column]+=calculated_num\n",
    "                deno_xi_Ux_sq=np.sum((calculated_num_current_column-df_useful.loc[3,useful_column])**2)\n",
    "                deno_yi_Uy_sq=np.sum((target_column-df_useful.loc[3,\"responder_6\"])**2)\n",
    "                deno_xi_Ux_2[useful_column]+=deno_xi_Ux_sq\n",
    "                deno_yi_Uy_2[useful_column]+=deno_yi_Uy_sq\n",
    "    for useful_column in df_useful:\n",
    "        denominator=(deno_xi_Ux_2[useful_column] ** 0.5) * (deno_yi_Uy_2[useful_column] ** 0.5)\n",
    "        if denominator != 0:\n",
    "            pearson_correlations[useful_column] = num[useful_column] / denominator\n",
    "        else:\n",
    "            pearson_correlations[useful_column] = np.nan \n",
    "    print(pearson_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"data/train.parquet\"\n",
    "df = pd.DataFrame()\n",
    "dataframes = []\n",
    "sum=0\n",
    "for file in os.listdir(train_dir):\n",
    "    df_file = os.path.join(train_dir, file, \"part-0.parquet\")\n",
    "    sum+=preprocess_data(df_file)\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usefulFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_useful = pd.read_csv(\"data/useful_columns.csv\")\n",
    "print(df_useful.loc[3,\"responder_6\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train=pd.read_parquet(\"data/train.parquet/partition_id=0/part-0.parquet\")\n",
    "df=pd.DataFrame(columns=df_train.columns)\n",
    "df.to_csv(\"data/useful_columns.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(target_column, batch_size=64, timesteps=60):\n",
    "    train_dir = \"data/train.parquet\"\n",
    "    \n",
    "    while True:  # Infinite loop to make the generator reusable\n",
    "        for file in os.listdir(train_dir):\n",
    "            # Construct full file path\n",
    "            file_path = os.path.join(train_dir, file)\n",
    "            \n",
    "            # Read the Parquet file\n",
    "            df = pd.read_parquet(file_path)\n",
    "            \n",
    "            # Select features and target\n",
    "            features = df.drop(columns=[target_column]).to_numpy()\n",
    "            target = df[target_column].to_numpy()\n",
    "            \n",
    "            # Scale features (optional, e.g., MinMaxScaler or StandardScaler)\n",
    "            # scaler = MinMaxScaler()\n",
    "            # features = scaler.fit_transform(features)\n",
    "            \n",
    "            X_batch, y_batch = [], []\n",
    "            \n",
    "            # Prepare data in sequences for LSTM\n",
    "            for i in range(len(features) - timesteps):\n",
    "                X = features[i:i+timesteps]\n",
    "                y = target[i+timesteps]\n",
    "                \n",
    "                X_batch.append(X)\n",
    "                y_batch.append(y)\n",
    "                \n",
    "                if len(X_batch) == batch_size:\n",
    "                    yield np.array(X_batch), np.array(y_batch)  # Return a batch\n",
    "                    X_batch, y_batch = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(50,activation=\"relu\", return_sequences=True, input_shape=input_shape),\n",
    "        LSTM(50,activation='relu',return_sequences=True),\n",
    "        Dense(1)  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse',metrics=['mae','accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(target_column, batch_size, timesteps):\n",
    "    train_dir = \"data/train.parquet\"\n",
    "    \n",
    "    while True:  # Infinite loop to make the generator reusable\n",
    "        for file in os.listdir(train_dir):\n",
    "            # Construct full file path\n",
    "            file_path = os.path.join(train_dir, file,\"part-0.parquet\")\n",
    "            \n",
    "            # Read the Parquet file\n",
    "            df = pd.read_parquet(file_path)\n",
    "            \n",
    "            # Select features and target\n",
    "            features = df.drop(columns=[target_column]).to_numpy()\n",
    "            target = df[target_column].to_numpy()\n",
    "            \n",
    "            X_batch, y_batch = [], []\n",
    "            \n",
    "            # Prepare data in sequences for LSTM\n",
    "            for i in range(len(features) - timesteps):\n",
    "                X = features[i:i + timesteps]\n",
    "                y = target[i + timesteps]\n",
    "                \n",
    "                X_batch.append(X)\n",
    "                y_batch.append(y)\n",
    "                \n",
    "                if len(X_batch) == batch_size:\n",
    "                    yield np.array(X_batch), np.array(y_batch)  # Return a batch\n",
    "                    X_batch, y_batch = [], []\n",
    "\n",
    "# Function to Calculate Steps Per Epoch\n",
    "def calculate_steps_per_epoch(parquet_dir, timesteps, batch_size):\n",
    "    total_samples = 0\n",
    "    for file in os.listdir(parquet_dir):\n",
    "        file_path = os.path.join(parquet_dir, file,\"part-0.parquet\")\n",
    "        total_samples += pq.ParquetFile(file_path).metadata.num_rows - timesteps\n",
    "    return total_samples // batch_size\n",
    "\n",
    "# LSTM Model\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(50, activation=\"relu\", return_sequences=True, input_shape=input_shape),\n",
    "        LSTM(50, activation=\"relu\"),\n",
    "        Dense(1)  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Main Script\n",
    "if __name__ == \"__main__\":\n",
    "    target_column = \"responder_6\"\n",
    "    batch_size = 4096\n",
    "    timesteps = 60\n",
    "    parquet_dir = \"data/train.parquet\"\n",
    "    \n",
    "    # Determine Input Shape (using the first file)\n",
    "    sample_file = os.listdir(parquet_dir)[0]\n",
    "    sample_df = pd.read_parquet(os.path.join(parquet_dir, sample_file))\n",
    "    input_shape = (timesteps, sample_df.shape[1] - 1)  # Exclude the target column\n",
    "\n",
    "    # Create the LSTM model\n",
    "    model = create_lstm_model(input_shape)\n",
    "\n",
    "    # Calculate steps per epoch\n",
    "    steps_per_epoch = calculate_steps_per_epoch(parquet_dir, timesteps, batch_size)\n",
    "\n",
    "    # Initialize data generator\n",
    "    generator = data_generator(target_column=target_column, batch_size=batch_size, timesteps=timesteps)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(generator, steps_per_epoch=steps_per_epoch, epochs=10)\n",
    "\n",
    "    # Save the model\n",
    "    model.save(\"lstm_model.h5\")\n",
    "    print(\"Model training complete. Saved as 'lstm_model.h5'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
